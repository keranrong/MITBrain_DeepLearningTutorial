{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basics.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keranrong/MITBrain_DeepLearningTutorial/blob/master/basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hpMoHIL9buxC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "**Authors:** Yen-Ling Kuo and Eugenio Piasini for the [Brains, Minds and Machines summer course 2018](http://cbmm.mit.edu/summer-school/2018).\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Yan0U4btlY0T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the hands-on deep learning tutorial series for Brains, Minds, and Machines summer course. The tutorial will guide you to implement a multi-layer perceptron using numpy and get familiar with the basic construct of PyTorch. If you are familiar with these topics, feel free to jump to the exercise section at the end or go to other modules.\n",
        "\n",
        "*  [Convolution Neural Network](https://colab.research.google.com/drive/1_QVMpGNXRzjU-n0beoHmmoxjGig8v94C)\n",
        "*  [Transfer Learning](https://colab.research.google.com/drive/15exgMLrj7azSMQeKXBYwuHZAdJdCQloh)\n",
        "*  [Recurrent Neural Network](https://colab.research.google.com/drive/1jR_DGoVDcxZ104onxTk2C7YeV7vTt1DV)"
      ]
    },
    {
      "metadata": {
        "id": "5U8QQzBMbNyW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementing a Multi-layer Perceptron"
      ]
    },
    {
      "metadata": {
        "id": "2vgQcVI_TFCm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### XOr Problem\n",
        "\n",
        "The XOr (exclusive or) problem is a classical probem in artificial neural network research. It uses a neural network to predict the output of a XOr logic gate given two binary inputs.\n",
        "\n",
        "Here, we relax the XOr problem a bit. Instead of binary inputs, we give inputs in real values. When the two input values have the same sign, the network should output 1; otherwise, the network should output 0.\n",
        "\n",
        "This is how the training data looks like in a 2D plot. The red dot means the output label is 0, the blue dot means the output label is 1."
      ]
    },
    {
      "metadata": {
        "id": "QdWhnDtxaWmf",
        "colab_type": "code",
        "outputId": "45213be9-8671-49b9-938c-b92b122d63e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# input of the MLP\n",
        "X = np.array([[1,1], [1,-1], [-1,-1], [-1,1]])\n",
        "# output of the MLP\n",
        "y = np.array([[0], [1], [0], [1]])\n",
        "\n",
        "# plot the training data\n",
        "fig, ax = plt.subplots()\n",
        "for i in range(y.shape[0]):\n",
        "  if y[i][0] == 0:\n",
        "    marker = 'ro'\n",
        "  else:\n",
        "    marker = 'bo'\n",
        "  ax.plot(X[i][0], X[i][1], marker)\n",
        "ax.axhline(y=0, color='k')\n",
        "ax.axvline(x=0, color='k')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.lines.Line2D at 0x7f6d360e8588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXPTNzBLiom9mlpD/D1P65PbSbDU5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f6d389b1fd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "f-ZLpixBrNsT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Clearly, these data points are not linearly separable. We can not find one single line to classify the points correctly.\n",
        "\n",
        "In this part, we will build a multi-layer perceptron with nonlinearity to make predictions."
      ]
    },
    {
      "metadata": {
        "id": "E7KhqYuNsO81",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Implementing Activation Functions\n",
        "\n",
        "There are several activation functions to choose from, including sigmoid, tanh, relu, etc. Here, we implemented the sigmoid function as an example. In order to do backpropagation, we need to implement its derivative as well."
      ]
    },
    {
      "metadata": {
        "id": "KU2KeWZqwU7I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Sigmoid Function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of Sigmoid Function\n",
        "def derivative_sigmoid(x):\n",
        "    return x * (1 - x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u_b3BojzuPdq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### A Simple MLP Architecture\n",
        "\n",
        "We design a simple MLP to solve this problem. The MLP consists of a hidden layer and an output layer to map the hidden vector to the output values. \n",
        "\n",
        "First, we initialize the weight and bias of each layer as well as the training epoch and learning rate. "
      ]
    },
    {
      "metadata": {
        "id": "cQkt7V2pwYW-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epoch = 5000 # number of training iterations\n",
        "learning_rate = 0.1\n",
        "\n",
        "# dimension of each layer\n",
        "d_in = X.shape[1] # number of features in the input dataset\n",
        "d_h = 3   # hidden layer\n",
        "d_out = 1 # output layer\n",
        "\n",
        "# weight and bias initialization\n",
        "wh = np.random.uniform(size=(d_in, d_h))\n",
        "bh = np.random.uniform(size=(1, d_h))\n",
        "wout = np.random.uniform(size=(d_h, d_out))\n",
        "bout = np.random.uniform(size=(1, d_out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dZ44Oe5hkjda",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For each training iteration, we run one forward pass to get the predicted value and compute the loss between the prediction and the real value. For simplicity, we use the differences between the two values as the loss function. Then, we can compute the gradients and finally update the weights and biases."
      ]
    },
    {
      "metadata": {
        "id": "J7yugsy3wgU5",
        "colab_type": "code",
        "outputId": "93e22efa-da27-4a0b-8474-16aebc46f93d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(epoch):\n",
        "    # Forward pass\n",
        "    h = sigmoid(X.dot(wh) + bh)\n",
        "    y_pred = sigmoid(h.dot(wout) + bout)\n",
        "    \n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).sum()\n",
        "    if i % 500 == 0:\n",
        "        print('Epoch', i, ':', loss)\n",
        "\n",
        "    # Backpropagation to compute gradients\n",
        "    grad_y_pred = (y - y_pred) * derivative_sigmoid(y_pred)\n",
        "    grad_wout = h.T.dot(grad_y_pred)\n",
        "    grad_bout = np.sum(grad_y_pred, axis=0, keepdims=True)\n",
        "    grad_h = grad_y_pred.dot(wout.T) * derivative_sigmoid(h)\n",
        "    grad_wh = X.T.dot(grad_h)\n",
        "    grad_bh = np.sum(grad_h, axis=0, keepdims=True)\n",
        "\n",
        "    # Update weights and biases\n",
        "    wout += grad_wout * learning_rate\n",
        "    bout += grad_bout * learning_rate\n",
        "    wh += grad_wh * learning_rate\n",
        "    bh += grad_bh * learning_rate\n",
        "    \n",
        "print('Prediction of training data:')\n",
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 : 1.0434065765915648\n",
            "Epoch 500 : 0.005132341091276804\n",
            "Epoch 1000 : 0.007237810333915795\n",
            "Epoch 1500 : 0.018516013578097512\n",
            "Epoch 2000 : 0.05017020251491022\n",
            "Epoch 2500 : 0.02252450245470422\n",
            "Epoch 3000 : 0.024550407887971692\n",
            "Epoch 3500 : 0.01985056306238825\n",
            "Epoch 4000 : 0.016307926030648998\n",
            "Epoch 4500 : 0.013871609465979373\n",
            "Prediction of training data:\n",
            "[[0.07657822]\n",
            " [0.92465901]\n",
            " [0.08418696]\n",
            " [0.92670571]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D75MnhxpmLg-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The predicted values are very close to the labels in the training data. Since they are real values, to get the binary output, we can simply threshold the output to decide which label we want to assign."
      ]
    },
    {
      "metadata": {
        "id": "PSTz3Y8vo5zb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Although it is possible to implement any kind of layers and optmization methods with numpy, there are several machine learning libraries available. So, it is easier to build neural networks of different architectures.\n",
        "\n",
        "TensorFlow, PyTorch, Keras, MxNet, Caffe, Theano are all popular deep learning framework. We will focus on PyTorch in the following tutorials.\n"
      ]
    },
    {
      "metadata": {
        "id": "8-_pfRvycFiH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introducing PyTorch\n",
        "\n",
        "PyTorch is an open source library for numerical computation using  computation graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. \n",
        "\n",
        "\n",
        "Similar to python programming, we can add and execute a node to the computation graph immediately. This property makes it easy to debug the code and inspect the values in the network.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Yuz0npgvorCH",
        "colab_type": "code",
        "outputId": "baaf68da-0986-4522-b185-a8331518f7e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yLD-V2P7vsol",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "PyTorch provides several useful modules.\n",
        "\n",
        "*  [ ```torch.nn```](https://pytorch.org/docs/stable/nn.html): This module provides the building blocks to create the networks, including the implementation of the various layers. \n",
        "*  [```torch.optim```](https://pytorch.org/docs/stable/optim.html): This module provides various optimization algorithms. Most commonly used methods are already supported.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "sxXmnREVnxYw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n5V07-rzkKZc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tensors\n",
        "\n",
        "Tensors are the leaf variables in the computation graph. By default, a tensor is initialized randomly. \n",
        "\n",
        "You can perform the operations defined in [```torch.Tensor```](https://pytorch.org/docs/stable/tensors.html) on the tensors."
      ]
    },
    {
      "metadata": {
        "id": "ksDjUZ9V7AC3",
        "colab_type": "code",
        "outputId": "b8b86b8c-37e1-4c68-95f3-7a51c492a4c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "A = torch.Tensor(5, 3)\n",
        "B = torch.rand(5, 3)\n",
        "\n",
        "print(A)\n",
        "print(B)\n",
        "print(A + B)\n",
        "print(torch.add(A, B))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.9291e-36, 0.0000e+00, 3.7835e-44],\n",
            "        [0.0000e+00,        nan, 0.0000e+00],\n",
            "        [1.3733e-14, 6.4069e+02, 4.3066e+21],\n",
            "        [1.1824e+22, 4.3066e+21, 6.3828e+28],\n",
            "        [3.8016e-39, 8.3383e-10, 0.0000e+00]])\n",
            "tensor([[0.9762, 0.6550, 0.8054],\n",
            "        [0.3523, 0.0591, 0.1281],\n",
            "        [0.8579, 0.9648, 0.2046],\n",
            "        [0.5063, 0.7308, 0.1397],\n",
            "        [0.0565, 0.2522, 0.5584]])\n",
            "tensor([[9.7622e-01, 6.5504e-01, 8.0541e-01],\n",
            "        [3.5234e-01,        nan, 1.2808e-01],\n",
            "        [8.5788e-01, 6.4165e+02, 4.3066e+21],\n",
            "        [1.1824e+22, 4.3066e+21, 6.3828e+28],\n",
            "        [5.6543e-02, 2.5221e-01, 5.5841e-01]])\n",
            "tensor([[9.7622e-01, 6.5504e-01, 8.0541e-01],\n",
            "        [3.5234e-01,        nan, 1.2808e-01],\n",
            "        [8.5788e-01, 6.4165e+02, 4.3066e+21],\n",
            "        [1.1824e+22, 4.3066e+21, 6.3828e+28],\n",
            "        [5.6543e-02, 2.5221e-01, 5.5841e-01]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "psFIDzYG9N3N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Numpy Bridge\n",
        "\n",
        "You can convert a tensor to a numpy array easily and vice versa. The torch Tensor and numpy array will share their underlying memory locations, and changing one will change the value of the other."
      ]
    },
    {
      "metadata": {
        "id": "SaTw_NLC8j43",
        "colab_type": "code",
        "outputId": "b0894e05-bed3-48d9-ac07-b2c12f1711aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# convert pytorch tensor to numpy\n",
        "a = torch.ones(5)\n",
        "print(a.numpy())\n",
        "a.numpy()[1] = 2\n",
        "print(a)\n",
        "\n",
        "# convert numpy to pytorch tensor\n",
        "b = np.ones(5)\n",
        "print(torch.from_numpy(b))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1. 1. 1.]\n",
            "tensor([1., 2., 1., 1., 1.])\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZdGO2GO5iBtd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Computing Gradient with Autograd\n",
        "\n",
        "PyTorch uses a technique called automatic differentiation. That is, we have a recorder that records what operations we have performed, and then it replays it backward to compute our gradients. This technique is especially powerful when building neural networks, as we save time on one epoch by calculating differentiation of the parameters at the forward pass itself.\n",
        "\n",
        "Once you finish your computation in the forward pass, you can call ```.backward()``` and have all the gradients computed automatically. You can access the gradient w.r.t. this tensor in ```.grad```."
      ]
    },
    {
      "metadata": {
        "id": "EEMqVESN7ZGc",
        "colab_type": "code",
        "outputId": "cce51971-8ea9-498c-e09e-5b2ead31f664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "y = x * 2\n",
        "print(y)\n",
        "out = y.mean()\n",
        "\n",
        "# compute gradients\n",
        "out.backward()\n",
        "# print gradients d(out)/dx\n",
        "print(x.grad.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.4563, -0.8237, -0.5198], requires_grad=True)\n",
            "tensor([-0.9126, -1.6473, -1.0397], grad_fn=<MulBackward>)\n",
            "tensor([0.6667, 0.6667, 0.6667])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bWptF0Jvsyy7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using GPU\n",
        "\n",
        "To train on the GPU, we need to move all tensors and layers to the GPU first. You can do this by calling ```.cuda()``` or create a tensor with a device location. If you have multiple GPUs, you can select which GPU you want to use by setting ```cuda:n``` where n is the device id of the GPU. "
      ]
    },
    {
      "metadata": {
        "id": "L83hXMOJ7Zqn",
        "colab_type": "code",
        "outputId": "b6da40b4-d076-4a94-f4d5-0a15dae77cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# move tensor to GPU\n",
        "if torch.cuda.is_available():\n",
        "    a = a.cuda()\n",
        "    print(a)\n",
        "    x = torch.randn(2, 3, device=torch.device(\"cuda:0\"))\n",
        "    print(x)\n",
        "\n",
        "# move back to CPU\n",
        "a = a.cpu()\n",
        "print(a)\n",
        "x = torch.randn(2, 3, device=torch.device(\"cpu\"))\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[ 0.7474, -0.4061,  1.5934],\n",
            "        [ 1.6632,  0.0018,  0.7008]], device='cuda:0')\n",
            "tensor([1., 2., 1., 1., 1.])\n",
            "tensor([[-0.0331,  1.8443,  0.7673],\n",
            "        [-2.3190,  1.1860, -0.2020]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mdBN-XDAiUaH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Building Model: Linear Regression"
      ]
    },
    {
      "metadata": {
        "id": "Dst3kvAa4mzi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we know how to do the basic computation using PyTorch. Let's start to build a simple linear regression model.\n",
        "\n",
        "First, we generate 100 x, y data points for training. The data points follow this formula: y = 3.0 * x + 1.0\n",
        "\n",
        "The goal is to regress W and b that fits the training data."
      ]
    },
    {
      "metadata": {
        "id": "WVmEBRDtnL_n",
        "colab_type": "code",
        "outputId": "d2361dd6-b3da-4000-f114-ebb909616670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "cell_type": "code",
      "source": [
        "x_train = np.random.rand(100).astype(np.float32).reshape(-1,1)\n",
        "y_correct = 3.0 * x_train + 1.0\n",
        "\n",
        "plt.plot(x_train, y_correct)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f6cc76e8320>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJOYU5CIiMff/aPTnV+tSH4gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f6d02966c50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "v7sLgrdF55GE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A model in PyTorch is a subclass of ```nn.Module```. There are several predefined layers and containers in the [ ```torch.nn```](https://pytorch.org/docs/stable/nn.html) module. We can use those to build more complex network structures.\n",
        "\n",
        "You can check out the layers we talked about in the lecture here:\n",
        "\n",
        "*   [```nn.Linear```](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear): fully connected layer\n",
        "*   [```nn.Conv2d```](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d): convolution layer\n",
        "* [```nn.MaxPool2d```](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d): pooling\n",
        "* [```nn.LSTM```](https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM): LSTM unit\n",
        "\n",
        "More complex neural networks are easily built using the predefined layers with the container classes ```Sequential``` and ```Concat```. \n",
        "\n",
        "The main method to implement in the ```nn.Module``` is ```forward```. It computes the output of the model given the input Tensor. The computation we put here defines the network architecture and how data flows between layers."
      ]
    },
    {
      "metadata": {
        "id": "UDCf-Bsbs1G9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A linear regression model is basically a single linear layer!\n",
        "\n",
        "We first define the layers we needed in ```__init__``` and then build the computation in ```forward```."
      ]
    },
    {
      "metadata": {
        "id": "nhTUIFa1mBOv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LinearRegressionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "\n",
        "        super(LinearRegressionModel, self).__init__() \n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qDcS92LctVbu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We then instantiate the model for training. We will use the mean squared error ([`nn.MSELoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss)) as loss function and optimize the network with stochastic gradient descent ([`torch.optim.SGD`](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)).\n",
        "\n",
        "We need to specify the parameters we want to optimize in the first argument of the selected optimizer. Usually, that should be all parameters in the model."
      ]
    },
    {
      "metadata": {
        "id": "G_DRNqNWmJmw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LinearRegressionModel(1, 1)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "learing_rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learing_rate)\n",
        "\n",
        "epochs = 2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ApWdrpsuqkD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Similar to what we did in the numpy example, at each training iteration, we perform one forward pass, get the loss, compute the gradients (using ```.backward()```), and update the parameters by taking a ```.step()``` with the optimizer.\n",
        "\n",
        "**Note** that we need to clear the gradients that are accumulated from previous training iterations first so we don't optimize against the old gradients stored in the variables. We can easily do it by calling ```optimizer.zero_grad()``` at the beginning of each iteration. "
      ]
    },
    {
      "metadata": {
        "id": "iWJJUZnjmVjs",
        "colab_type": "code",
        "outputId": "78bc7030-3a40-4c43-bb43-368713e365d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    inputs = torch.from_numpy(x_train)\n",
        "    labels = torch.from_numpy(y_correct)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model.forward(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 100 == 0:\n",
        "        print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, loss 7.82508659362793\n",
            "epoch 100, loss 0.23652684688568115\n",
            "epoch 200, loss 0.1393134593963623\n",
            "epoch 300, loss 0.10546277463436127\n",
            "epoch 400, loss 0.08004981279373169\n",
            "epoch 500, loss 0.060762207955121994\n",
            "epoch 600, loss 0.04612170159816742\n",
            "epoch 700, loss 0.03500886261463165\n",
            "epoch 800, loss 0.026573505252599716\n",
            "epoch 900, loss 0.02017076313495636\n",
            "epoch 1000, loss 0.015310725197196007\n",
            "epoch 1100, loss 0.011621679179370403\n",
            "epoch 1200, loss 0.00882150512188673\n",
            "epoch 1300, loss 0.006696018390357494\n",
            "epoch 1400, loss 0.005082668736577034\n",
            "epoch 1500, loss 0.003858013777062297\n",
            "epoch 1600, loss 0.0029284381307661533\n",
            "epoch 1700, loss 0.0022228388115763664\n",
            "epoch 1800, loss 0.0016872496344149113\n",
            "epoch 1900, loss 0.0012807432794943452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rkv5XxhTxf8Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After the model is trained, we can apply the ```forward``` method to get the prediction of any data point.\n",
        "\n",
        "We can also visualize it to see what the predictions look like!"
      ]
    },
    {
      "metadata": {
        "id": "dVv7qRWcmtLD",
        "colab_type": "code",
        "outputId": "84ba3cb6-bb36-444b-a02d-e02db2524a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "cell_type": "code",
      "source": [
        "predicted = model.forward(torch.from_numpy(x_train)).data.numpy()\n",
        "\n",
        "plt.plot(x_train, y_correct, 'go', label='from data', alpha=0.5)\n",
        "plt.plot(x_train, predicted, label='prediction', alpha=0.5)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6cc6c5da58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f6cc6c346a0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "TOvsDfZ3yCcC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In case anything goes wrong and you want to check the learned parameters of your model, you can inspect the `weight` and `bias` of a layer as follows. "
      ]
    },
    {
      "metadata": {
        "id": "7U1l_qu1UB_C",
        "colab_type": "code",
        "outputId": "a5c3d2a3-a4db-46a2-db32-f866ab297c7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print(model.linear.weight)\n",
        "print(model.linear.bias)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[2.8933]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([1.0523], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OZItfQbKkaOO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise: Revisit the XOr Example"
      ]
    },
    {
      "metadata": {
        "id": "lr3Rc0blycby",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We just went through basic operations of PyTorch and learned how to build a simple model with it. Let's try to apply what we just learned to build the XOr classifier we talked about at the beginning of the tutorial.\n",
        "\n",
        "This time, I give your more data points ranging between -10 and 10 for trainig."
      ]
    },
    {
      "metadata": {
        "id": "D3OvIBzZzXKJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(1)\n",
        "\n",
        "x_train = []\n",
        "y_correct = []\n",
        "for i in range(100):\n",
        "    x0 = random.randint(-10, 10)\n",
        "    x1 = random.randint(-10, 10)\n",
        "    label = 0 if x0 * x1 > 0 else 1\n",
        "    x_train.append([x0, x1])\n",
        "    y_correct.append(label)\n",
        "x_train = np.asarray(x_train)\n",
        "y_correct = np.asarray(y_correct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EWeHO8mgzHuV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now is your job to implement the structure of the classifier. You can use the architecture in our first example, which consists a hidden and an output linear layer.\n",
        "\n",
        "To apply sigmoid function, you can use [`torch.sigmoid`](https://pytorch.org/docs/stable/torch.html#torch.sigmoid)."
      ]
    },
    {
      "metadata": {
        "id": "GEpZGXiCz9O3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class XorClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "\n",
        "        super(XorClassifier, self).__init__() \n",
        "        # TODO: please initialize the layers here\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: please build your forward pass here\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZkeRbaEa2xB1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Different from what we did before, we want to have our output layer to output two values to indicate the probability distribution over the two possible labels instead of one real value for the label. \n",
        "\n",
        "So, we set the output dimension to 2 and use cross entropy as loss function ([`nn.CrossEntropyLoss()`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)) because it is useful when training a classification problem with N classes. Don't worry if you're not familiar with the cross entropy, we will have more expanations in the next tutorial.\n",
        "\n",
        "We also try a different optimizer this time. [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) is an adaptive optimization algorithm. You can try to compare it with SGD as well!"
      ]
    },
    {
      "metadata": {
        "id": "4A207iKQ0V0E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = XorClassifier(2, 2)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nwlmcJ4B2cgJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's complete the final part of the training script to learn the classifier!"
      ]
    },
    {
      "metadata": {
        "id": "QvUkHJGA0qHZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    inputs = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
        "    labels = torch.from_numpy(y_correct)\n",
        "    \n",
        "    # TODO: please complete the training script\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tZcAm8Jx2tqj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since we're predicting the distribution over labels, we need to take the index of the max value as the predicted label.\n",
        "\n",
        "Then, we can draw the predictions in a 2D plot. If your model lears well, you will see most dots in the 1st and 3rd quadrants are red, and the dots in the 2nd and 4th quadrants are blue."
      ]
    },
    {
      "metadata": {
        "id": "_0BUGKtMRE3c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "outputs = model.forward(torch.from_numpy(x_train).type(torch.FloatTensor))\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "for i in range(y_correct.shape[0]):\n",
        "    if predicted[i] == 0:\n",
        "        marker = 'ro'\n",
        "    else:\n",
        "        marker = 'bo'\n",
        "    ax.plot(x_train[i][0], x_train[i][1], marker)\n",
        "ax.axhline(y=0, color='k')\n",
        "ax.axvline(x=0, color='k')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0gtNP-lfY4KL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Congrats! \n",
        "\n",
        "Let's move to the next tutorial: [Convolutional neural network](https://colab.research.google.com/drive/1_QVMpGNXRzjU-n0beoHmmoxjGig8v94C)."
      ]
    }
  ]
}